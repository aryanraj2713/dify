from api.core.model_manager import ModelManager
from api.core.model_runtime.entities.model_entities import ModelType
from api.core.rag.splitter.fixed_text_splitter import FixedRecursiveCharacterTextSplitter
from api.core.rag.splitter.semantic_text_splitter import SemanticTextSplitter
from api.core.rag.models.document import Document
from api.core.rag.index_processor.index_processor_base import BaseIndexProcessor
from api.core.rag.index_processor.processor.parent_child_index_processor import ParentChildIndexProcessor

# Sample text with distinct semantic sections
sample_text = """
# Renewable Energy Technologies

Renewable energy is derived from natural sources that are replenished at a higher rate than they are consumed. Sunlight, wind, rain, tides, waves, and geothermal heat are all sustainable sources that can be harnessed to generate power.

## Solar Energy

Solar power is captured in two main ways: photovoltaic (PV) panels and solar thermal collectors. PV panels convert sunlight directly into electricity, while solar thermal collectors use the sun's energy to heat water or air for domestic or industrial use.

The efficiency of solar panels has dramatically improved in recent years, with modern panels reaching over 20% efficiency. This improvement, coupled with decreasing costs, has made solar energy increasingly competitive with fossil fuels in many regions.

## Wind Energy

Wind turbines convert the kinetic energy of moving air into mechanical power, which can then be converted into electricity. These turbines range from small residential units to large utility-scale wind farms with hundreds of turbines.

Wind energy is one of the fastest-growing renewable energy sources globally. Its cost-effectiveness has improved significantly, with the levelized cost of wind power now comparable to or lower than that of conventional fossil fuel plants in many locations.

## Hydroelectric Power

Hydroelectric power is generated by harnessing the energy of falling or flowing water. Large-scale hydropower projects typically involve dams that create reservoirs, while run-of-river systems utilize the natural flow of rivers without significant water storage.

While hydropower is a mature technology with high efficiency, the environmental impacts of large dams can be significant, affecting river ecosystems and sometimes requiring the displacement of human communities.
"""

def test_rag_pipeline_with_semantic_chunking():
    print("Testing RAG pipeline with semantic chunking")
    print("-" * 50)
    
    # Initialize model instances
    model_manager = ModelManager()
    try:
        llm_model_instance = model_manager.get_default_model_instance(ModelType.LLM)
        print(f"Using LLM: {llm_model_instance.model} from {llm_model_instance.provider}")
    except Exception as e:
        print(f"Could not get LLM model instance: {e}")
        print("Semantic chunking requires an LLM - this test will fall back to fixed chunking")
        llm_model_instance = None
    
    try:
        embedding_model_instance = model_manager.get_default_model_instance(ModelType.TEXT_EMBEDDING)
        print(f"Using embedding model: {embedding_model_instance.model} from {embedding_model_instance.provider}")
    except Exception as e:
        print(f"Could not get embedding model instance: {e}")
        print("Warning: Embedding model is required for the full RAG pipeline")
        embedding_model_instance = None
    
    # Create parent document
    parent_doc = Document(page_content=sample_text)
    
    print("\n1. Testing Parent-Child Processing with Semantic Chunking")
    
    # Simulate process rule configuration with semantic chunking
    process_rule = {
        "mode": "custom",
        "rules": {
            "pre_processing_rules": [],
            "parent_mode": "paragraph",
            "segmentation": {
                "separator": "\\n\\n",
                "max_tokens": 500,
                "chunk_overlap": 50
            },
            "subchunk_segmentation": {
                "separator": "\\n",
                "max_tokens": 200,
                "chunk_overlap": 20
            },
            "chunking_strategy": "semantic"
        }
    }
    
    # Create parent-child processor
    parent_child_processor = ParentChildIndexProcessor()
    
    # Transform the document
    try:
        transformed_docs = parent_child_processor.transform(
            [parent_doc],
            process_rule=process_rule,
            embedding_model_instance=embedding_model_instance,
            llm_model_instance=llm_model_instance
        )
        
        print(f"\nParent-Child processing completed")
        print(f"Number of parent documents: {len(transformed_docs)}")
        
        if transformed_docs:
            parent = transformed_docs[0]
            print(f"Parent document length: {len(parent.page_content)} chars")
            print(f"Number of child chunks: {len(parent.children) if parent.children else 0}")
            
            if parent.children:
                print("\nChild Chunks:")
                for i, child in enumerate(parent.children):
                    print(f"\nChild {i+1} ({len(child.page_content)} chars):")
                    print(child.page_content[:100] + "..." if len(child.page_content) > 100 else child.page_content)
                    
                print("\nChecking for natural semantic boundaries in child chunks:")
                natural_boundaries = [".", "!", "?"]
                complete_sentences = sum(1 for child in parent.children if child.page_content.strip()[-1] in natural_boundaries)
                print(f"Complete sentences: {complete_sentences}/{len(parent.children)} child chunks")
                
        else:
            print("No documents were returned from transformation")
    
    except Exception as e:
        print(f"Error during parent-child processing: {e}")
    
    # Test fallback mechanism
    print("\n2. Testing Fallback Mechanism")
    if llm_model_instance:
        try:
            # Create semantic splitter with invalid parameters to force fallback
            semantic_splitter = SemanticTextSplitter(
                llm_model_instance=llm_model_instance,
                chunk_size=20,  # Too small to trigger errors
                chunk_overlap=5,
            )
            
            # Attempt to split with parameters that would cause issues
            semantic_chunks = semantic_splitter.split_text(sample_text)
            print(f"Fallback handling worked, got {len(semantic_chunks)} chunks")
        except Exception as e:
            print(f"Error during fallback test: {e}")
    else:
        print("Skipping fallback test as no LLM is available")

if __name__ == "__main__":
    test_rag_pipeline_with_semantic_chunking() 